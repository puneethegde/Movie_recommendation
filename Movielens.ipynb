{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhF29mT3MYXX",
        "outputId": "4f572b77-3fd2-4bad-a2b8-e47c07ed7a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-23 13:20:17--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4924029 (4.7M) [application/zip]\n",
            "Saving to: ‘ml-100k.zip’\n",
            "\n",
            "ml-100k.zip         100%[===================>]   4.70M  21.4MB/s    in 0.2s    \n",
            "\n",
            "2023-09-23 13:20:17 (21.4 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
            "\n",
            "Archive:  ml-100k.zip\n",
            "   creating: ml-100k/\n",
            "  inflating: ml-100k/allbut.pl       \n",
            "  inflating: ml-100k/mku.sh          \n",
            "  inflating: ml-100k/README          \n",
            "  inflating: ml-100k/u.data          \n",
            "  inflating: ml-100k/u.genre         \n",
            "  inflating: ml-100k/u.info          \n",
            "  inflating: ml-100k/u.item          \n",
            "  inflating: ml-100k/u.occupation    \n",
            "  inflating: ml-100k/u.user          \n",
            "  inflating: ml-100k/u1.base         \n",
            "  inflating: ml-100k/u1.test         \n",
            "  inflating: ml-100k/u2.base         \n",
            "  inflating: ml-100k/u2.test         \n",
            "  inflating: ml-100k/u3.base         \n",
            "  inflating: ml-100k/u3.test         \n",
            "  inflating: ml-100k/u4.base         \n",
            "  inflating: ml-100k/u4.test         \n",
            "  inflating: ml-100k/u5.base         \n",
            "  inflating: ml-100k/u5.test         \n",
            "  inflating: ml-100k/ua.base         \n",
            "  inflating: ml-100k/ua.test         \n",
            "  inflating: ml-100k/ub.base         \n",
            "  inflating: ml-100k/ub.test         \n"
          ]
        }
      ],
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
        "!unzip ml-100k.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "data_dir = '/content/ml-100k/'\n",
        "\n",
        "# Load the user ratings data into a Pandas DataFrame\n",
        "ratings_file = data_dir + 'u.data'  # This file contains user ratings\n",
        "ratings_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
        "ratings_df = pd.read_csv(ratings_file, sep='\\t', names=ratings_cols)\n",
        "\n",
        "# Load movie information (if available)\n",
        "movies_file = data_dir + 'u.item'  # This file contains movie information\n",
        "movies_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url']\n",
        "movies_df = pd.read_csv(movies_file, sep='|', names=movies_cols, encoding='latin-1')\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "# Drop unnecessary columns (e.g., timestamp, video_release_date, imdb_url)\n",
        "ratings_df = ratings_df[['user_id', 'movie_id', 'rating']]\n",
        "\n",
        "# Handle missing values (if any)\n",
        "ratings_df.dropna(inplace=True)\n",
        "\n",
        "# Convert categorical variables to numerical representations (e.g., user_id and movie_id)\n",
        "ratings_df['user_id'] = ratings_df['user_id'].astype('category').cat.codes\n",
        "ratings_df['movie_id'] = ratings_df['movie_id'].astype('category').cat.codes\n",
        "\n",
        "# Optionally, you can merge movie information into the ratings DataFrame\n",
        "# For example, to add movie titles to the ratings DataFrame:\n",
        "ratings_df = pd.merge(ratings_df, movies_df[['movie_id', 'title']], on='movie_id')\n",
        "\n",
        "# Now, you have a preprocessed DataFrame ready for building your recommendation system\n",
        "print(ratings_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvhMWK8TMyfF",
        "outputId": "7b9732fd-662e-4d4a-b43e-603db4b8f0e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user_id  movie_id  rating  title\n",
            "0      307         0       4      0\n",
            "1      307         0       4      0\n",
            "2      307         0       4      0\n",
            "3      307         0       4      0\n",
            "4      307         0       4      0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-d175a5a85e48>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  ratings_df.dropna(inplace=True)\n",
            "<ipython-input-3-d175a5a85e48>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  ratings_df['user_id'] = ratings_df['user_id'].astype('category').cat.codes\n",
            "<ipython-input-3-d175a5a85e48>:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  ratings_df['movie_id'] = ratings_df['movie_id'].astype('category').cat.codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df.to_csv('preprocessed_ratings.csv', index=False)"
      ],
      "metadata": {
        "id": "Xk6zIAFXPNDE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-surprise -qqq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq5_1GE7Nv-C",
        "outputId": "6c7dee3a-fbad-420a-d4a8-dc752f72bd97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/772.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/772.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from surprise import SVD\n",
        "from surprise import Dataset\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "from surprise import Reader\n",
        "\n",
        "ratings_df = pd.read_csv('/content/preprocessed_ratings.csv')\n",
        "\n",
        "# Building the Recommendation System with Matrix Factorization (SVD)\n",
        "\n",
        "# Create a Surprise Dataset object from the DataFrame\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(ratings_df[['user_id', 'movie_id', 'rating']], reader)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVD model\n",
        "svd_model = SVD(n_factors=100, n_epochs=20, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "svd_model.fit(trainset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u2KvPCWNx6P",
        "outputId": "21e23b6b-b58b-4ae6-ef10-1e5e872ab39e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7999326c3b20>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_id_to_predict = 6\n",
        "user_ratings = ratings_df[ratings_df['user_id'] == user_id_to_predict]\n",
        "movies_not_rated_by_user = ratings_df[~ratings_df['movie_id'].isin(user_ratings['movie_id'])]\n",
        "\n",
        "\n",
        "movies_to_predict = list(movies_not_rated_by_user['movie_id'])\n",
        "\n",
        "predictions = [svd_model.predict(user_id_to_predict, movie_id) for movie_id in movies_to_predict]\n",
        "\n",
        "# Sort the predictions by estimated rating (higher first)\n",
        "sorted_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)\n",
        "\n",
        "# Get the top N recommended movie IDs\n",
        "top_n = 10\n",
        "top_movie_ids = [prediction.iid for prediction in sorted_predictions[:top_n]]\n",
        "\n",
        "# Get the movie titles corresponding to the recommended movie IDs\n",
        "recommended_movies = movies_df[movies_df['movie_id'].isin(top_movie_ids)]['title']\n",
        "\n",
        "print(\"Top {} movie recommendations for user {}:\".format(top_n, user_id_to_predict))\n",
        "for idx, movie in enumerate(recommended_movies, start=1):\n",
        "    print(\"{}. {}\".format(idx, movie))\n",
        "\n",
        "\n",
        "test_predictions = svd_model.test(testset)\n",
        "rmse = accuracy.rmse(test_predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test set: {:.4f}\".format(rmse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmjTbGbeP62Y",
        "outputId": "c38b486f-d50a-49f6-f4d7-bfa5d1b7c5d4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 movie recommendations for user 6:\n",
            "1. 0\n",
            "2. 0\n",
            "3. 0\n",
            "4. 0\n",
            "5. 0\n",
            "6. 0\n",
            "7. 1\n",
            "8. 0\n",
            "9. 0\n",
            "10. 0\n",
            "11. 0\n",
            "12. 0\n",
            "13. 0\n",
            "14. 0\n",
            "15. 0\n",
            "16. 0\n",
            "17. 0\n",
            "18. 0\n",
            "19. 0\n",
            "20. 0\n",
            "21. 0\n",
            "22. 0\n",
            "23. 0\n",
            "24. 0\n",
            "25. 0\n",
            "26. 0\n",
            "27. 1\n",
            "28. 0\n",
            "29. 1\n",
            "30. 0\n",
            "31. 0\n",
            "32. 0\n",
            "33. 0\n",
            "34. 0\n",
            "35. 0\n",
            "36. 0\n",
            "37. 0\n",
            "38. 0\n",
            "39. 0\n",
            "40. 0\n",
            "41. 0\n",
            "42. 0\n",
            "43. 0\n",
            "44. 0\n",
            "45. 0\n",
            "46. 0\n",
            "47. 0\n",
            "48. 0\n",
            "49. 0\n",
            "50. 0\n",
            "51. 0\n",
            "52. 0\n",
            "53. 0\n",
            "54. 0\n",
            "55. 0\n",
            "56. 0\n",
            "57. 0\n",
            "58. 0\n",
            "59. 0\n",
            "60. 0\n",
            "61. 0\n",
            "62. 0\n",
            "63. 0\n",
            "64. 1\n",
            "65. 0\n",
            "66. 0\n",
            "67. 0\n",
            "68. 0\n",
            "69. 0\n",
            "70. 0\n",
            "71. 0\n",
            "72. 0\n",
            "73. 0\n",
            "74. 0\n",
            "75. 0\n",
            "76. 0\n",
            "77. 0\n",
            "78. 0\n",
            "79. 0\n",
            "80. 0\n",
            "81. 0\n",
            "82. 0\n",
            "83. 0\n",
            "84. 0\n",
            "85. 0\n",
            "86. 0\n",
            "87. 0\n",
            "88. 1\n",
            "89. 0\n",
            "90. 0\n",
            "91. 0\n",
            "92. 0\n",
            "93. 0\n",
            "94. 0\n",
            "95. 0\n",
            "96. 0\n",
            "97. 0\n",
            "98. 0\n",
            "99. 0\n",
            "100. 0\n",
            "101. 0\n",
            "102. 0\n",
            "103. 0\n",
            "104. 0\n",
            "105. 0\n",
            "106. 0\n",
            "107. 0\n",
            "108. 0\n",
            "109. 0\n",
            "110. 0\n",
            "111. 0\n",
            "112. 0\n",
            "113. 0\n",
            "114. 0\n",
            "115. 0\n",
            "116. 0\n",
            "117. 0\n",
            "118. 0\n",
            "119. 0\n",
            "120. 0\n",
            "121. 0\n",
            "122. 0\n",
            "123. 0\n",
            "124. 0\n",
            "125. 0\n",
            "126. 0\n",
            "127. 0\n",
            "128. 0\n",
            "129. 0\n",
            "130. 0\n",
            "131. 0\n",
            "132. 0\n",
            "133. 0\n",
            "134. 0\n",
            "135. 0\n",
            "136. 0\n",
            "137. 0\n",
            "138. 0\n",
            "139. 0\n",
            "140. 0\n",
            "141. 0\n",
            "142. 0\n",
            "143. 0\n",
            "144. 0\n",
            "145. 0\n",
            "146. 0\n",
            "147. 0\n",
            "148. 0\n",
            "149. 0\n",
            "150. 0\n",
            "151. 0\n",
            "152. 0\n",
            "153. 0\n",
            "154. 0\n",
            "155. 0\n",
            "156. 0\n",
            "157. 0\n",
            "158. 0\n",
            "159. 0\n",
            "160. 0\n",
            "161. 0\n",
            "162. 0\n",
            "163. 0\n",
            "164. 0\n",
            "165. 0\n",
            "166. 0\n",
            "167. 0\n",
            "168. 0\n",
            "169. 0\n",
            "170. 0\n",
            "171. 0\n",
            "172. 0\n",
            "173. 0\n",
            "174. 0\n",
            "175. 0\n",
            "176. 0\n",
            "177. 0\n",
            "178. 0\n",
            "179. 0\n",
            "180. 0\n",
            "181. 0\n",
            "182. 0\n",
            "183. 0\n",
            "184. 0\n",
            "185. 0\n",
            "186. 0\n",
            "187. 0\n",
            "188. 0\n",
            "189. 0\n",
            "190. 0\n",
            "191. 0\n",
            "192. 0\n",
            "193. 0\n",
            "194. 0\n",
            "195. 0\n",
            "196. 0\n",
            "197. 0\n",
            "198. 0\n",
            "199. 0\n",
            "200. 0\n",
            "201. 0\n",
            "202. 0\n",
            "203. 0\n",
            "204. 0\n",
            "205. 0\n",
            "206. 0\n",
            "207. 0\n",
            "208. 0\n",
            "209. 0\n",
            "210. 0\n",
            "211. 0\n",
            "212. 0\n",
            "213. 0\n",
            "214. 0\n",
            "215. 0\n",
            "216. 0\n",
            "217. 0\n",
            "218. 0\n",
            "219. 0\n",
            "220. 0\n",
            "221. 0\n",
            "222. 0\n",
            "223. 0\n",
            "224. 0\n",
            "225. 0\n",
            "226. 0\n",
            "227. 0\n",
            "228. 0\n",
            "229. 0\n",
            "230. 0\n",
            "231. 0\n",
            "232. 0\n",
            "233. 0\n",
            "234. 0\n",
            "235. 0\n",
            "236. 0\n",
            "237. 0\n",
            "238. 0\n",
            "239. 0\n",
            "240. 0\n",
            "241. 0\n",
            "242. 0\n",
            "243. 0\n",
            "244. 0\n",
            "245. 0\n",
            "246. 0\n",
            "247. 0\n",
            "RMSE: 0.3983\n",
            "Root Mean Squared Error (RMSE) on test set: 0.3983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8-rs3IiVP8XA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}